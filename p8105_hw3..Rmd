---
title: "P8105_homework3"
author: "Danny Nguyen"
date: "2023-10-11"
output: github_document
editor_options: 
  chunk_output_type: console
---

# Question 1 

## Load dataset
```{r setup}
library(p8105.datasets)
library(tidyverse)
library(patchwork)
data("instacart")
```
Description: Dataset 'instacart' has `r nrow(instacart)` rows and `r ncol(instacart)` columns, which makes it generally a large-sized dataset. This data is from an online grocery service that allows you to shop online from local stores in NYC. Some of its notable variables include identifiers (order, product,customers), order_number (the order sequence number for this user), order_dow (the day of the week on which the order was placed), order_hour_of_day (the hour of the day on which the order was placed), product_name (name of the product), etc.

## How many aisles are there, and which aisles are the most items ordered from?
```{r}
instacart %>% 
  summarise(aisle=n_distinct(aisle_id))
```
There are 134 distinct aisles.

```{r}
instacart %>%
  group_by(aisle_id)%>%
  summarise(sum=n())%>%
  arrange(desc(sum))
```
From this, aisle with id 83 (fresh vegetables) is the one with the most items ordered from (150609 ordered items). Coming right after it, aisle 24 (fresh fruits) and aisle 123 (packaged vegetables fruits) are at 2nd and 3rd places respectively. 

## Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.
```{r}
instacart %>%
  group_by(aisle)%>%
  summarise(num_items=n())%>%
  filter(num_items>10000)%>% 
  ggplot(aes(x= reorder(aisle, -num_items), y=num_items)) +
  geom_bar(stat="identity", fill="red4")+
  coord_flip()+
  labs(title = "Number of Ordered Items in Each Aisle",
       x = "Aisle Name",
       y = "Number of Ordered Items from Each Aisle") +
  theme_minimal()
```
This bar graph shows number of ordered items in each aisle that is organized in an ascending order. And this graph delivers the same results as we found out earlier, that fresh vegetables, fresh fruits, and packaged vegetables fruits are the ones with the most number of ordered items. 

## Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r, message=FALSE}
top3popular_items <- instacart %>%
  filter (aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits")%>%
  group_by(aisle, product_name)%>%
  summarise(num_items=n())%>% 
  arrange(aisle, desc(num_items))%>%
  group_by(aisle)%>%
  top_n(3)

knitr::kable(top3popular_items,caption = "Top 3 Popular Items")
```
Here is the table showing  the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits” with their number of times each item is ordered. In "baking ingredients", there are light brown sugar, pure baking soda, and cane sugar. In dog food care, there are Snack Sticks Chicken & Rice Recipe Dog Treats, Organix Chicken & Brown Rice Recipe, and Small Dog Biscuits. And in packaged vegetables fruits, there are Organic Baby Spinach, Organic Raspberries, and Organic Blueberries. 

## Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table)
```{r, message = FALSE}
apple_coffee = instacart %>% 
  filter (product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>%
  summarise(mean_hour= mean(order_hour_of_day, na.rm=TRUE))%>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )%>%
  rename(Monday = "0", Tuesday = "1", Wednesday = "2", Thursday = "3", Friday = "4", Saturday = "5", Sunday = "6")

knitr::kable(apple_coffee, caption = "Mean Hour in each Day of the Week Ordering Selected Items", digits = 2)
```
This table shows 2 products of coffee ice cream and Pink Lady apples with their mean hours that they are being ordered during the week. 

# Question 2
## Load dataset
```{r setup2}
data("brfss_smart2010")
```

## Data Cleaning 
```{r}
brfss <- brfss_smart2010 %>% 
  rename(location_abbr = Locationabbr,
         location_desc = Locationdesc,
         data_source = DataSource,
         class_id = ClassId,
         topic_id = TopicId,
         location_id = LocationID,
         question_id = QuestionID,
         resp_id = RESPID,
         geo_location = GeoLocation)%>%
  janitor::clean_names() %>%
  filter( topic == "Overall Health" | 
          response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>%
  mutate(response = factor(response, 
                           levels = c("Poor", "Fair", "Good", "Very good", "Excellent"),
                           ordered = TRUE))
```
After performing data cleaning, the subset of the orginal data contains `r nrow(brfss)` rows and `r ncol(brfss)` columns. 

## In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r}
brfss %>% 
  filter( year == "2002")%>% 
  group_by(location_abbr)%>%
  summarise(num_location=n_distinct(location_desc))%>% 
  filter(num_location >=7)%>%
  pull(location_abbr)
```
For the 2002, there are 6 states that were observed at 7 or more locations including: "CT" "FL" "MA" "NC" "NJ" "PA"

```{r}
brfss %>% 
  filter( year == "2010")%>% 
  group_by(location_abbr)%>%
  summarise(num_location=n_distinct(location_desc))%>% 
  filter(num_location >=7)%>%
  pull(location_abbr)
```
However, in 2010, there are 14 states that were observed at 7 or more locations including: "CA" "CO" "FL" "MA" "MD" "NC" "NE" "NJ" "NY" "OH" "PA" "SC" "TX" "WA". 

## Construct a dataset that is limited to "Excellent" responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).
```{r} 
excellent <- brfss %>% 
  filter (response == "Excellent") %>%
  select (year, location_abbr, data_value) %>% 
  group_by (location_abbr, year) %>% 
  summarise(mean_data_value = round(mean(data_value, na.rm=TRUE),2))

ggplot(excellent, aes (x=year, y=mean_data_value, group=location_abbr)) +
  geom_line(aes(color=location_abbr), alpha=0.5) + theme_minimal() + labs(title = "Average value for each state annually",
                                                                          y = "Average value")
```
This is the "spaghetti" graph showing the trends of “Excellent” responses over time among different from each state across years. Each state is coded with its own color (referred to legend on the graph). Overall, it is a very busy graph due to large number of states, and no specific trends. However, we can observe a dense overlapping of lines around the 20-25 mark implying that there are a significant number of states having their "Excellent” responses average in this range during various years.


## Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State
```{r}
plot_2006 = brfss |>
  filter(location_abbr=="NY",year==2006)|>
  ggplot(aes(x =location_desc , y = data_value,fill=response)) + 
  geom_bar(position="dodge",stat="identity") +  
  labs(title = "Distribution of data_value in NY State in 2006") + 
  theme(plot.title = element_text(size = 10), axis.text.x = element_text(angle = 20, hjust = 1),legend.position = "none") 

plot_2010 = brfss |>
  filter(location_abbr=="NY",year==2010)|>
  ggplot(aes(x =location_desc , y = data_value,fill=response)) + 
  geom_bar(position="dodge",stat="identity") +  
  labs(title = "Distribution of data_value in NY State in 2010") +
  theme(plot.title = element_text(size = 10), axis.text.x = element_text(angle = 20, hjust = 1))

plot_2006+plot_2010
```
This is two-panel plot showing data values between 2006 and 2010 across locations in NY.
First of all, it is worth to notice that there are some counties (i.e. Bronx) do not have data in 2006, but do have data in 2010. While there is variation among counties, the overall distribution appears somewhat consistent between the two years. While many counties exhibit bars of similar heights between the two years (e.g., Kings County, Queens County), some show noticeable changes. For instance, the “Very good” category appears to decrease in some counties from 2006 to 2010, while others see an increase.

# Question 3

## Data Import, Tidy, & Wrangling
```{r}
accel <- read_csv("C:/Users/duong/OneDrive/Desktop/MAILMAN 3/Data 1/homework3_/nhanes_accel.csv") %>%
  janitor::clean_names() %>% 
  mutate(seqn = as.integer(seqn))

demographic <- read_csv("C:/Users/duong/OneDrive/Desktop/MAILMAN 3/Data 1/homework3_/nhanes_covar.csv", skip = 4) %>%
  janitor::clean_names()%>%
  na.omit() %>%
  filter(age >= 21) %>%
  mutate(seqn = as.integer(seqn), sex=as.factor(sex), education = as.factor(education))

merge_df <- inner_join(accel, demographic, by = "seqn") 

merge_df <-merge_df %>%
  select(seqn, sex, age, bmi, education, everything()) %>% 
  mutate(sex = if_else(sex == "1", "male", "female"), 
         education = if_else(education == "1", "Less than high school", 
                             if_else(education == "2", "High school equivalent", 
                                     if_else(education=="3", "More than high school", NA))))
```
Description: Dataset 'merge_df' is a merge between 2 given datasets, which has `r nrow(merge_df)` rows and `r ncol(merge_df)` columns.This data shows demographic data and their accelerometer data of MIMS values for each minute of a 24-hour day starting at midnight for 228 satisfied-criteria participants. 

## Create table 
```{r}
education_sex <- table(merge_df$sex, merge_df$education)

knitr::kable(education_sex, caption = "The number of men and women in each education category")
```
From this table, we can observe that the numbers of female and male are relatively similar in "Less than high school" and "More than high school" catergories, but there are significantly more male than female in "High school equivalent" (35>>23).

## Plot 
```{r}
ggplot(merge_df, aes(x = factor(education, levels= c("Less than high school", "High school equivalent", "More than high school"), ordered = TRUE), y = age, fill = sex)) +
  geom_boxplot() +
  labs(title = "Age Distribution by Sex and Education",
       x = "Education Level",
       y = "Age",
       fill = "Sex") +
    theme_minimal()
```
The people from "More than high school" are the youngest on average, compared to the other groups, while "Less than high school" is the ones being the oldest on average.
The group with the largest gap in age distribution between male and female is "High school equivalent". In this group, female are older than male. 


## Total activity Plot 

```{r}
merge_df %>%
  mutate(rowsums = select(., -c(1:5)) %>% 
           rowSums(na.rm = TRUE))%>%
  ggplot(aes(x = age, y = rowsums, color = sex)) +
  geom_point(aes(shape = sex), alpha = 0.5) + 
  geom_smooth(method = "lm", se = FALSE) + 
  facet_wrap(~ factor(education, levels= c("Less than high school", "High school equivalent", "More than high school"), ordered = TRUE)) + 
  labs(title = "Total Activity with Age by Sex and Education Level",
       x = "Age",
       y = "Total Activity",
       color = "Sex",
       shape = "Sex") +
  theme_minimal()
```
Description: 
There is a consistene trend across all 3 education levels that there is a negative correlation between the total activity and age. This can imply that the total activity decrease with age, similarly between both sexes. The group in "Less than high school" shows more total activity than the other two groups for both male and female. For groups of "High school equivalent" and "More than high school", women always have more total activity than men at the same age.

## 3-panel plot
```{r}
merge_df_hour <- merge_df %>%
  pivot_longer(min1:min1440, 
               names_prefix = "min",
               names_to = "minute", 
               values_to = "activity") %>%
  mutate(minute = as.integer(minute))

ggplot(merge_df_hour, aes(x = minute, y = activity, color = sex)) +
  geom_line(alpha = 0.5) +
  geom_smooth(se = FALSE) +
  facet_wrap(~ factor(education, levels= c("Less than high school", "High school equivalent", "More than high school"), ordered = TRUE), ncol = 1) +
  scale_x_continuous(name = "Hour", breaks = seq(0, 1440, 60), labels = 0:24) +
  labs(title = "24-hour Activity Time Courses by Education Level",
       y = "Activity",
       color = "Sex") 
```
Description:
Across 3 education groups, there is very little evidence of  gender-related variation in total activity as indicated by the smooth trends. In all 3 groups, the majority of activity is observed from  9 am to 8 pm. Notably, the daily activity time follows a ranking of "Less than high school" > "High school equivalent" > "More than high school".