---
title: "P8105_homework3"
author: "Danny Nguyen"
date: "2023-10-11"
output: html_document
editor_options: 
  chunk_output_type: console
---

# Question 1 

## Load dataset
```{r setup}
library(p8105.datasets)
library(tidyverse)
data("instacart")
```
Description: Dataset 'instacart' has `r nrow(instacart)` rows and `r ncol(instacart)` columns, which makes it generally a large-sized dataset. This data is from an online grocery service that allows you to shop online from local stores in NYC. Some of its notable variables include identifiers (order, product,customers), order_number (the order sequence number for this user), order_dow (the day of the week on which the order was placed), order_hour_of_day (the hour of the day on which the order was placed), product_name (name of the product), etc.

## How many aisles are there, and which aisles are the most items ordered from?
```{r}
instacart %>% 
  summarise(aisle=n_distinct(aisle_id))
```
There are 134 distinct aisles. 

```{r}
instacart %>%
  group_by(aisle_id)%>%
  summarise(sum=n())%>%
  arrange(desc(sum))
```
From this, aisle with id 83 (fresh vegetables) is the one with the most items ordered from (150609 ordered items). Coming right after it, aisle 24 (fresh fruits) and aisle 123 (packaged vegetables fruits) are at 2nd and 3rd places respectively. 

## Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.
```{r}
instacart %>%
  group_by(aisle)%>%
  summarise(num_items=n())%>%
  filter(num_items>10000)%>% 
  ggplot(aes(x= reorder(aisle, -num_items), y=num_items)) +
  geom_bar(stat="identity", fill="red4")+
  coord_flip()+
  labs(title = "Number of Ordered Items in Each Aisle",
       x = "Aisle Name",
       y = "Number of Ordered Items from Each Aisle") +
  theme_minimal()
```
This bar graph shows number of ordered items in each aisle that is organized in an ascending order. 

## Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r, message=FALSE}
top3popular_items <- instacart %>%
  filter (aisle == "baking ingredients" | aisle == "dog food care" | aisle == "packaged vegetables fruits")%>%
  group_by(aisle, product_name)%>%
  summarise(num_items=n())%>% 
  arrange(aisle, desc(num_items))%>%
  group_by(aisle)%>%
  top_n(3)

knitr::kable(top3popular_items,caption = "Top 3 Popular Items")
```
Here is the table showing  the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits” with their number of times each item is ordered. 

## Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table)
```{r, message = FALSE}
apple_coffee = instacart %>% 
  filter (product_name == "Pink Lady Apples" | product_name == "Coffee Ice Cream") %>% 
  group_by(product_name, order_dow) %>%
  summarise(mean_hour= mean(order_hour_of_day, na.rm=TRUE))%>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )%>%
  rename(Monday = "0", Tuesday = "1", Wednesday = "2", Thursday = "3", Friday = "4", Saturday = "5", Sunday = "6")

knitr::kable(apple_coffee, caption = "Mean Hour in each Day of the Week Ordering Selected Items", digits = 2)
```

# Question 2
## Load dataset
```{r setup2}
data("brfss_smart2010")
```

## Data Cleaning 
```{r}
brfss <- brfss_smart2010 %>% 
  rename(location_abbr = Locationabbr,
         location_desc = Locationdesc,
         data_source = DataSource,
         class_id = ClassId,
         topic_id = TopicId,
         location_id = LocationID,
         question_id = QuestionID,
         resp_id = RESPID,
         geo_location = GeoLocation)%>%
  janitor::clean_names() %>%
  filter( topic == "Overall Health" | 
          response == "Excellent" | response == "Very good" | response == "Good" | response == "Fair" | response == "Poor") %>%
  mutate(response = factor(response, 
                           levels = c("Poor", "Fair", "Good", "Very good", "Excellent"),
                           ordered = TRUE))
```
After performing data cleaning, the subset of the orginal data contains `r nrow(brfss)` rows and `r ncol(brfss)` columns. 

## In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r}
brfss %>% 
  filter( year == "2002")%>% 
  group_by(location_abbr)%>%
  summarise(num_location=n_distinct(location_desc))%>% 
  filter(num_location >=7)%>%
  pull(location_abbr)
```
For the 2002, there are 6 states that were observed at 7 or more locations including: "CT" "FL" "MA" "NC" "NJ" "PA"

```{r}
brfss %>% 
  filter( year == "2010")%>% 
  group_by(location_abbr)%>%
  summarise(num_location=n_distinct(location_desc))%>% 
  filter(num_location >=7)%>%
  pull(location_abbr)
```
However, in 2010, there are 14 states that were observed at 7 or more locations including: "CA" "CO" "FL" "MA" "MD" "NC" "NE" "NJ" "NY" "OH" "PA" "SC" "TX" "WA". 

## Construct a dataset that is limited to "Excellent" responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).
```{r} 
excellent <- brfss %>% 
  filter (response == "Excellent") %>%
  select (year, location_abbr, data_value) %>% 
  group_by (location_abbr, year) %>% 
  summarise(mean_data_value = round(mean(data_value, na.rm=TRUE),2))

ggplot(excellent, aes (x=year, y=mean_data_value, group=location_abbr)) +
  geom_line(aes(color=location_abbr), alpha=0.5) + theme_minimal() + labs(title = "Average value for each state annually",
                                                                          y = "Average value")
```
This is the "spaghetti" graph showing the average value of variable "data_value" from each state across years. Each state is coded with its own color (referred to legend on the graph). Overall, it is a very busy graph due to large number of states, and no specific trends. 

## Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State
```{r}
ny_2006_2010 <- brfss %>%
  filter( year == "2006" | year == "2010") %>% 
  filter (location_abbr == "NY")

ggplot(ny_2006_2010, aes (x=response, y=data_value))+
  geom_boxplot() +
  facet_wrap(~year) + theme_minimal() +
  labs(title = "Distribution of data_value across types of responses in NY in 2006&2010")
```
This is the boxplot plots for distribution of data_value across 5 types of responses in NY in 2006 & 2010. 

